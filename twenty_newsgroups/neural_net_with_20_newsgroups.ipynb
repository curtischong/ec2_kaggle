{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n",
      "ERROR (theano.gpuarray): pygpu was configured but could not be imported or is too old (version 0.7 or higher required)\n",
      "NoneType\n"
     ]
    }
   ],
   "source": [
    "#fized warning with sudo apt install libblas-dev\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.layers import Activation, Conv1D, Dense, Embedding, Flatten, Input, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.base import get_data_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/bin/python3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pygpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e37a42e8aab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpygpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pygpu'"
     ]
    }
   ],
   "source": [
    "#http://deeplearning.net/software/theano/NEWS.html\n",
    "#ik my versions all work\n",
    "#theano-1.0.2\n",
    "#certifi-2018.4.16\n",
    "#pygpu-0.7.6\n",
    "#conda-4.5.4\n",
    "import pygpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset='all', shuffle=True, download_if_missing=False)\n",
    "\n",
    "texts = dataset.data # Extract text\n",
    "target = dataset.target # Extract target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "18846\n",
      "157\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "10\n",
      "rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "print (len(texts))\n",
    "print (len(target))\n",
    "print (len(texts[0].split()))\n",
    "print (texts[0])\n",
    "print (target[0])\n",
    "print (dataset.target_names[target[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size) # Setup tokenizer\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts) # Generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1595, 151, 82, 20, 13, 7, 122, 706, 82, 50, 13]]\n",
      "18846\n",
      "160\n",
      "[14, 19415, 455, 559, 15, 29, 2552, 1240, 5609, 33, 322, 767, 2175, 2121, 871, 1343, 32, 251, 88, 77, 84, 12087, 455, 559, 15, 7, 122, 228, 63, 3, 2552, 1240, 20, 517, 3490, 50, 1, 1393, 3, 61, 437, 3, 1507, 50, 1, 1302, 2552, 3027, 3, 1, 2701, 309, 7, 122, 243, 16334, 175, 5, 4, 243, 19416, 268, 7, 122, 194, 2, 296, 37, 337, 2, 369, 4389, 22, 4, 243, 3, 7286, 12, 1, 2552, 349, 30, 20, 1502, 137, 2701, 1382, 90, 7, 397, 5987, 74, 2025, 13, 130, 56, 8, 140, 215, 90, 93, 1457, 770, 1963, 56, 8, 97, 4, 308, 9186, 1857, 2, 1306, 6, 1, 2327, 6760, 115, 348, 5987, 21, 4, 308, 3, 1857, 6, 1, 365, 658, 3, 467, 185, 1, 2552, 20, 194, 2, 1985, 1, 66, 3, 3215, 608, 7, 26, 132, 8755, 19, 2, 131, 1, 3280, 2000, 1, 1151, 1457, 770, 283, 2552, 1222]\n"
     ]
    }
   ],
   "source": [
    "print (tokenizer.texts_to_sequences(['hello world, how are you? I am fine, how about you?']))\n",
    "\n",
    "print (len(sequences))\n",
    "print (len(sequences[0]))\n",
    "print (sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 179,209 unique words.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found {:,} unique words.'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from ratnam andrew cmu edu subject pens fans reactions organization post office carnegie mellon pittsburgh pa lines 12 nntp posting host po4 andrew cmu edu i am sure some of pens fans are pretty confused about the lack of any kind of posts about the recent pens massacre of the devils actually i am bit puzzled too and a bit relieved however i am going to put an end to non relief with a bit of praise for the pens man they are killing those devils worse than i thought jagr just showed you why he is much better than his regular season stats he is also a lot fo fun to watch in the playoffs bowman should let jagr have a lot of fun in the next couple of games since the pens are going to beat the out of jersey anyway i was very disappointed not to see the islanders lose the final regular season game pens rule "
     ]
    }
   ],
   "source": [
    "# Create inverse index mapping numbers to words\n",
    "inv_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "# Print out text again\n",
    "for w in sequences[0]:\n",
    "    x = inv_index.get(w)\n",
    "    print(x,end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292.4769712405816, 666.9329063050876)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the average length of a text\n",
    "avg = sum(map(len, sequences)) / len(sequences)\n",
    "\n",
    "# Get the standard deviation of the sequence length\n",
    "std = np.sqrt(sum(map(lambda x: (len(x) - avg)**2, sequences)) / len(sequences))\n",
    "\n",
    "avg,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 2, 3]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences([[1,2,3]], maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "data = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (18846, 100)\n",
      "Shape of labels: (18846, 20)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "labels = to_categorical(np.asarray(target))\n",
    "print('Shape of data:', data.shape)\n",
    "print('Shape of labels:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400,000 word vectors in GloVe.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = '../../models/glove' # This is the folder with the dataset\n",
    "\n",
    "embeddings_index = {} # We create a dictionary of word -> embedding\n",
    "\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0] # The first value is the word, the rest are the values of the embedding\n",
    "        embedding = np.asarray(values[1:], dtype='float32') # Load embedding\n",
    "        embeddings_index[word] = embedding # Add embedding to our embedding dictionary\n",
    "\n",
    "print('Found {:,} word vectors in GloVe.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.043084   0.53233    0.54254   -0.076952  -0.29673    0.52986\n",
      "  0.21379    0.15789   -0.3952    -0.91889   -0.6585     0.68706\n",
      "  0.10821   -0.10694   -0.3401     1.044      0.12775    0.51157\n",
      "  0.60314    0.71366   -0.5374     0.37737    0.12186    0.60891\n",
      "  0.50107    2.0215    -0.47318    0.46953    0.12542    0.60207\n",
      "  0.11007    0.37587    1.0137    -0.2478     0.65748    0.12801\n",
      " -0.57647   -0.25754    0.62426    0.010864  -0.40681    0.16173\n",
      " -0.84695   -0.24603    0.29078    0.8546    -0.067021   0.69331\n",
      " -0.71545   -0.25184   -0.74741   -0.26507    0.4873     0.41991\n",
      " -0.86741   -0.5235    -0.44774   -0.044584   0.033836   0.29909\n",
      "  0.73754    0.81651    0.69431    0.80453    0.29276   -0.025244\n",
      " -0.30453   -0.34329    0.11933   -0.29655    0.1072    -0.18946\n",
      "  0.18501   -0.7548    -0.25628    0.34438   -0.016743   0.0040503\n",
      "  0.39342    0.99404   -0.32159   -0.49434    0.41708   -0.011019\n",
      " -0.16613   -0.20839    0.28152   -0.82996    0.79839    0.61645\n",
      "  0.31537   -0.27629   -0.54592    0.23026    0.023473  -0.15934\n",
      " -1.4389    -0.75359    0.5149    -0.52552  ]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print (embeddings_index['frog'])\n",
    "print (len(embeddings_index['frog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1249743\n",
      "6.7943544\n"
     ]
    }
   ],
   "source": [
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['toad']))\n",
    "print (np.linalg.norm(embeddings_index['frog'] - embeddings_index['man']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100 # We use 100 dimensional glove vectors\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(vocab_size, len(word_index)) # How many words are there actually\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "\n",
    "# The vectors need to be in the same position as their index. \n",
    "# Meaning a word with token 1 needs to be in the second row (rows start with zero) and so on\n",
    "\n",
    "# Loop over all words in the word index\n",
    "for word, i in word_index.items():\n",
    "    # If we are above the amount of words we want to use we do nothing\n",
    "    if i >= vocab_size: \n",
    "        continue\n",
    "    # Get the embedding vector for the word\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # If there is an embedding vector, put it in the embedding matrix\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 98, 128)           38528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 30, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 8, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 2,172,564\n",
      "Trainable params: 172,564\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, \n",
    "                    embedding_dim, \n",
    "                    input_length=max_length, \n",
    "                    weights = [embedding_matrix], \n",
    "                    trainable = False))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15076 samples, validate on 3770 samples\n",
      "Epoch 1/2\n",
      "15076/15076 [==============================] - 154s 10ms/step - loss: 0.1507 - acc: 0.9528 - val_loss: 0.1209 - val_acc: 0.9570\n",
      "Epoch 2/2\n",
      "15076/15076 [==============================] - 153s 10ms/step - loss: 0.1013 - acc: 0.9636 - val_loss: 0.0979 - val_acc: 0.9651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f10400d9e10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(data, labels, validation_split=0.2, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 i'm posting this for a friend but you can e mail questions to me at cc bellcore com however the best way to get your questions answered is to call the phone number listed for sale 1991 corrado 2 2 coupe low mileage approx 28 000 miles 5 speed manual 7 speaker factory stereo system new all weather 205 sun roof ac red speed activated extra set of tires equipped with factory winter package heated seats mirrors and security system with 2 all records documentation service car mint condition must sacrifice at 11 000 or best offer call 908 "
     ]
    }
   ],
   "source": [
    "example = data[450] # get the tokens\n",
    "\n",
    "# Print tokens as text\n",
    "for w in example:\n",
    "    x = inv_index.get(w)\n",
    "    print(x,end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get prediction\n",
    "pred = model.predict(example.reshape(1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'misc.forsale'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output predicted category\n",
    "dataset.target_names[np.argmax(pred)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
